#!/usr/bin/env python

import argparse
import itertools
from math import sqrt
import os.path
import re
from subprocess import Popen, PIPE
import sys
import threading
import time

# The perf-tests project directory.
proj_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), "..")) 
sbt_cmd = "sbt/sbt"

parser = argparse.ArgumentParser(description='Run Spark or Shark peformance tests. Before running, '
    'edit the supplied configuration file.')

parser.add_argument('--config-file', help='override default location of config file, must be a '
    'python file that ends in .py', default="%s/config/config.py" % proj_dir)

args = parser.parse_args()
assert args.config_file.endswith(".py"), "config filename must end with .py"

# Check if the config file exists.
assert os.path.isfile(args.config_file), ("Please create a config file called %s (you probably "
    "just want to copy and then modify %s/config/config.py.template)" % 
    (args.config_file, proj_dir))

print "Detected project directory: %s" % proj_dir
# Import the configuration settings from the config file.
print("Adding %s to sys.path" % os.path.dirname(args.config_file))
sys.path.append(os.path.dirname(args.config_file))
print("running 'import %s'" % os.path.basename(args.config_file).split(".")[0])
exec("import %s" % os.path.basename(args.config_file).split(".")[0])

# Determine which projects and tests to build based on user-specified tests and boolean flags from
# the config file.
# Build Hive and Shark if there are Shark tests specified.
has_shark_tests = (len(config.SHARK_TESTS) > 0)
should_prep_shark = has_shark_tests and not config.SHARK_SKIP_PREP
should_prep_hive = should_prep_shark and not config.HIVE_SKIP_PREP

# Spark will be built if there are Spark or Shark tests.
has_spark_tests = (len(config.SPARK_TESTS) > 0)
should_prep_spark = (has_shark_tests or has_spark_tests) and not config.SPARK_SKIP_PREP

# Only build the perf test sources that will be used.
should_prep_shark_tests = has_shark_tests and not config.SHARK_SKIP_TEST_PREP
should_prep_spark_tests = has_spark_tests and not config.SPARK_SKIP_TEST_PREP

# Do disk warmup only if there are tests to run.
should_warmup_disk = (has_shark_tests or has_spark_tests) and not config.SKIP_DISK_WARMUP

# Check that commit ID's are specified in config_file.
if should_prep_spark:
    assert config.SPARK_COMMIT_ID is not "", \
        ("Please specify SPARK_COMMIT_ID in %s" % args.config_file)
if should_prep_spark:
    assert config.SHARK_COMMIT_ID is not "", \
        ("Please specifiy SHARK_COMMIT_ID in %s" % args.config_file)

# Run shell command and ignore output.
def run_cmd(cmd, exit_on_fail=True):
    if cmd.find(";") != -1:
        print("***************************")
        print("WARNING: the following command contains a semicolon which may cause non-zero return "
            "values to be ignored. This isn't necessarily a problem, but proceed with caution!")
    print(cmd)
    return_code = Popen(cmd, stdout=sys.stderr, shell=True).wait()
    if exit_on_fail:
        if return_code != 0:
            print "The following shell command finished with a non-zero returncode (%s): %s" % (
                return_code, cmd)
            sys.exit(-1)
    return return_code

# Run several commands in parallel, waiting for them all to finish.
# Expects an array of tuples, where each tuple consists of (command_name, exit_on_fail).
def run_cmds_parallel(commands):
    threads = []
    for (cmd_name, exit_on_fail) in commands:
        thread = threading.Thread(target=run_cmd, args=(cmd_name, exit_on_fail))
        thread.start()
        threads = threads + [thread]
    for thread in threads:
        thread.join()

# Return a command running cmd_name on host with proper SSH configs.
def make_ssh_cmd(cmd_name, host):
    return "ssh -o StrictHostKeyChecking=no -o ConnectTimeout=5 %s '%s'" % (host, cmd_name)

# Return a command which copies the supplied directory to the given host.
def make_rsync_cmd(dir_name, host):
    return ('rsync --delete -e "ssh -o StrictHostKeyChecking=no -o ConnectTimeout=5" -az "%s/" '
        '"%s:%s"') % (dir_name, host, os.path.abspath(dir_name))

# Delete all files in the given directory on the specified hosts.
def clear_dir(dir_name, hosts):
    assert dir_name != "" and dir_name != "/", ("Attempted to delete directory '%s/*', halting "
        "rather than deleting entire file system.") % dir_name
    if config.PROMPT_FOR_DELETES:
        response = raw_input("\nAbout to remove all files and directories under %s on %s, is "
            "this ok? [y, n] " % (dir_name, hosts))
        if response != "y":
            return
    run_cmds_parallel([(make_ssh_cmd("rm -r %s/*" % dir_name, host), False) for host in hosts])

# Ensures that no executors are running on Spark slaves. Executors can continue to run for some
# time after a shutdown signal is given due to cleaning up temporary files.
def ensure_spark_stopped_on_slaves(slaves):
    stop = False
    while not stop:
        cmd = "ps -ef |grep -v grep |grep ExecutorBackend"
        ret_vals = map(lambda s: run_cmd(make_ssh_cmd(cmd, s), False), slaves)
        if 0 in ret_vals:
            print "Spark is still running on some slaves ... sleeping for 10 seconds"
            time.sleep(10)
        else:
            stop = True

# Get a list of slaves by parsing the slaves file in SPARK_CONF_DIR.
slaves_file_raw = open("%s/slaves" % config.SPARK_CONF_DIR, 'r').read().split("\n")
slaves_list = filter(lambda x: not x.startswith("#") and not x is "", slaves_file_raw)

# If a cluster is already running from the Spark EC2 scripts, try shutting it down.
if os.path.exists("%s/bin/stop-all.sh" % config.SPARK_HOME_DIR):
    run_cmd("%s/bin/stop-all.sh" % config.SPARK_HOME_DIR)

# If a cluster is already running from an earlier test, try shutting it down.
if os.path.exists("%s/spark/bin/stop-all.sh" % proj_dir):
    print("Stopping Spark standalone cluster...")
    run_cmd("%s/spark/bin/stop-all.sh" % proj_dir)

# Ensure all shutdowns have completed (no executors are running).
ensure_spark_stopped_on_slaves(slaves_list)
# Allow some extra time for slaves to fully terminate.
time.sleep(5) 

# Prepare Spark (unless arg was passed that causes this to be skipped).
if should_prep_spark:
    # Assumes that the preexisting 'spark' directory is valid.
    if not os.path.isdir("spark"):
        # Clone Spark.
        print("Git cloning Spark...")
        run_cmd("git clone %s spark" % config.SPARK_GIT_REPO)
        run_cmd("cd spark; git config --add remote.origin.fetch "
            "'+refs/pull/*/head:refs/remotes/origin/pr/*'")

    # Fetch updates.
    os.chdir("spark")
    print("Updating Spark repo...")
    run_cmd("git fetch")

    # Build Spark.
    print("Cleaning Spark and building branch %s. This may take a while...\n" %
        config.SPARK_COMMIT_ID)
    run_cmd("git clean -f -d -x")
    
    if config.SPARK_MERGE_COMMIT_INTO_MASTER:
        run_cmd("git reset --hard master")
        run_cmd("git merge %s -m ='Merging %s into master.'" %
            (config.SPARK_COMMIT_ID, config.SPARK_COMMIT_ID))
    else:
        run_cmd("git reset --hard %s" % config.SPARK_COMMIT_ID)

    run_cmd("%s clean package" % sbt_cmd)

    # Copy Spark configuration files to new directory.
    print("Copying all files from %s to %s/spark/conf/" % (config.SPARK_CONF_DIR, proj_dir))
    assert os.path.exists("%s/spark-env.sh" % config.SPARK_CONF_DIR), \
        "Could not find required file %s/spark-env.sh" % config.SPARK_CONF_DIR
    assert os.path.exists("%s/slaves" % config.SPARK_CONF_DIR), \
        "Could not find required file %s/slaves" % config.SPARK_CONF_DIR
    run_cmd("cp %s/* %s/spark/conf/" % (config.SPARK_CONF_DIR, proj_dir))

    # Change back to 'proj_dir' directory.
    os.chdir("..")
else:
    # TODO(andy): Make this check for the jar we will be including on the
    #             classpath (as part of work Patrick is doing), instead of
    #             just looking for spark/target.
    assert os.path.exists(
        "%s/spark/target" % proj_dir), ("You chose to skip Spark prep, but we can't since " +
        "%s/spark/target does not exist, so Spark needs to be rebuilt/packaged." % proj_dir)
    print("Skipping preparation tasks for Spark (i.e, git clone or update, checkout, build, " +
        "copy conf files).")

# Prepare Hive (unless an arg was passed that causes this to be skipped).
if should_prep_hive:
    # Assumes that the preexisting 'hive' directory is valid.
    if not os.path.isdir("hive"):
        # Clone the Hive repository.
        print("Git cloning Hive...")
        run_cmd("git clone %s hive" % config.HIVE_GIT_REPO)

    # Build Hive.
    os.chdir("hive")
    run_cmd("ant clean package")

    # Change back to 'proj_dir' directory.
    os.chdir("..")
else:
    assert os.path.exists(
        "%s/hive/build/dist/lib" % proj_dir), ("You chose to skip Hive prep, but we can't since " +
        "%s/hive/build/dist/lib does not exist, so Hive needs to be rebuilt/packaged." % proj_dir)
    print("Skipping preparation tasks for Hive (i.e, git clone, build).")

# Prepare Shark (unless arg was passed that caues this to be skipped).
if should_prep_shark:
    if not os.path.isdir("shark"):
        # Clone the Shark repository.
        print("Git cloning Shark...")
        run_cmd("git clone %s shark" % config.SHARK_GIT_REPO)
        run_cmd("cd shark; git config --add remote.origin.fetch "
            "'+refs/pull/*/head:refs/remotes/origin/pr/*'")

    # Fetch updates.
    os.chdir("shark")
    print("Updating Shark repo...")
    run_cmd("git fetch")

    # Build Shark.
    print("Cloning Shark and building branch %s. This may take a while...\n" %
        config.SHARK_COMMIT_ID)
    run_cmd("git clean -f -d -x")

    if config.SHARK_MERGE_COMMIT_INTO_MASTER:
        run_cmd("git reset --hard master")
        run_cmd("git merge %s -m ='Merging %s into master.'" %
            config.SHARK_COMMIT_ID, config.SHARK_COMMIT_ID)
    else:
        run_cmd("git reset --hard %s" % config.SHARK_COMMIT_ID)

    run_cmd("%s clean package" % sbt_cmd)

    # Copy Shark configuration files from the SHARK_CONF_DIR specified in config.py.
    print("Copying all files from %s to %s/shark/conf/" % (config.SHARK_CONF_DIR, proj_dir))
    assert os.path.exists("%s/shark-env.sh" % config.SHARK_CONF_DIR), \
        "Could not find required file %s/shark-env.sh" % config.SHARK_CONF_DIR
    run_cmd("cp %s/* %s/shark/conf/" % (config.SHARK_CONF_DIR, proj_dir))

    # Change from 'perf-tests/shark' back to the 'proj_dir' directory.
    os.chdir("..")
else:
    assert os.path.exists(
        "%s/shark/target" % proj_dir), ("You chose to skip Shark prep, but we can't since "
        "%s/shark/target does not exist, so Shark needs to be rebuilt/packaged." % proj_dir)
    print("Skipping preparation tasks for Shark (i.e, git clone or update, checkout, build, copy).")

# Build the tests for each project based on copies cloned to the 'proj_dir'.
spark_work_dir = "%s/spark/work" % proj_dir
if os.path.exists(spark_work_dir):
    # Clear the 'perf-tests/spark/work' directory beforehand, since that may contain scheduler logs
    # from previous jobs. The directory could also contain a spark-perf-tests-assembly.jar that may
    # interfere with subsequent 'sbt assembly' for Spark perf.
    clear_dir(spark_work_dir, ["localhost"])

print("Building perf tests...")
if should_prep_spark_tests:
    run_cmd("cd %s/spark-tests; %s clean assembly" % (proj_dir, sbt_cmd))
else:
    spark_test_jar_path = "%s/spark-tests/target/spark-perf-tests-assembly.jar" % proj_dir
    assert os.path.exists(spark_test_jar_path), ("You tried to skip packaging the Spark perf " +
        "tests, but %s was not already present") % spark_test_jar_path
if should_prep_shark_tests:
    run_cmd("cd %s/shark-tests; %s clean assembly" % (proj_dir, sbt_cmd))
else:
    shark_test_jar_path = "%s/shark-tests/target/shark-perf-tests-assembly.jar" % proj_dir
    assert os.path.exists(shark_test_jar_path), ("You tried to skip packaging the Shark perf " +
        "tests, but %s was not already present") % shark_test_jar_path

# Sync the whole directory to the slaves.
print("Syncing the test directory to the slaves.")
run_cmds_parallel([(make_rsync_cmd(proj_dir, slave), True) for slave in slaves_list])

# Start our Spark cluster.
print("Starting a Spark standalone cluster to use for testing...")
assert os.path.exists("%s/spark/bin/start-all.sh" % proj_dir), ("%s/spark/bin/start-all.sh must "
    "exist") % proj_dir
run_cmd("%s/spark/bin/start-all.sh" % proj_dir)
time.sleep(5) # Starting the cluster takes a little time so give it a second.

# Set Spark Java Options (from config.py)
new_env = os.environ.copy()

new_env["SPARK_HOME"] = "%s/spark" % proj_dir

# Search for 'spark.local.dir' in spark-env.sh, or shark-env.sh if Shark is also being built. Note
# that for the latter case, 'spark.local.dir' in shark-env.sh takes precedence.
spark_local_dirs = "" 
path_to_env_file = ""
if has_shark_tests:
    # Store Hive metastore and warehouse subdirectories created by each Shark PerfTest.
    perf_test_warehouses_dir = "%s/test-warehouses" % proj_dir
    new_env["PERF_TEST_WAREHOUSES"] = perf_test_warehouses_dir
    path_to_env_file = "%s/shark-env.sh" % config.SHARK_CONF_DIR
else:
    path_to_env_file = "%s/spark-env.sh" % config.SPARK_CONF_DIR

env_file_content = open(path_to_env_file, 'r').read()
re_result = re.search(r'spark.local.dir=([^"]*)"', env_file_content)
if re_result:
    spark_local_dirs = re_result.group(1).split(",")
else:
    sys.exit("ERROR: These scripts require you to explicitly set spark.local.dir in spark-env.sh "
        "(or shark-env.sh) so that it can be cleaned. The way we check this is pretty picky, "
        "specifically we try to find the following string in spark-env.sh (or shark-env.sh): "
        "spark.local.dir=ONE_OR_MORE_DIRNAMES\" so you will want a line like this: "
        "SPARK_JAVA_OPTS+=\" -Dspark.local.dir=/tmp\"")

if should_warmup_disk:
    for local_dir in spark_local_dirs:
        # Strip off any trailing whitespace(s) so that the clear commands below can work properly.
        local_dir = local_dir.rstrip()

        bytes_to_write = config.DISK_WARMUP_BYTES
        bytes_per_file = bytes_to_write / config.DISK_WARMUP_FILES
        gen_command = "dd if=/dev/urandom bs=%s count=1 | split -a 5 -b %s - %s/random" % (
            bytes_to_write, bytes_per_file, local_dir)
        # Ensures the directory exists.
        dir_command = "mkdir -p %s" % local_dir
        clear_command = "rm -f %s/*" % local_dir

        print("Generating test data for %s, this may take some time" % local_dir)
        all_hosts = slaves_list + ["localhost"]
        run_cmds_parallel([(make_ssh_cmd(dir_command, host), True) for host in all_hosts])
        run_cmds_parallel([(make_ssh_cmd(gen_command, host), True) for host in all_hosts])
        clear_dir(local_dir, all_hosts)        

# Some utility functions to calculate useful stats on test output.
def average(in_list):
    return sum(in_list) / len(in_list)

def variance(in_list):
    variance = 0
    for x in in_list:
        variance = variance + (average(in_list) - x) ** 2
    return variance / len(in_list)

# Run all tests specified in 'tests_to_run', a list of 5-element tuples. See the 'Test Setup'
# section in config.py.template for more info.
# Results are written as CSVs to 'output_filename'.
def run_tests(scala_cmd_classpath, tests_to_run, test_group_name, output_filename):
    out_file = open(output_filename, 'w')
    num_tests_to_run = len(tests_to_run)

    output_divider_string = "\n--------------------------------------------------------------------"
    print(output_divider_string)
    print("Running %d tests in %s.\n" % (num_tests_to_run, test_group_name))

    for short_name, test_cmd, scale_factor, java_opt_sets, opt_sets in tests_to_run:
        print(output_divider_string)
        print("Running test command: '%s' ..." % test_cmd)
        # Run a test for all combinations of the OptionSets given, then capture
        # and print the output.
        java_opt_set_arrays = [i.to_array(scale_factor) for i in java_opt_sets]
        opt_set_arrays = [i.to_array(scale_factor) for i in opt_sets]
        for java_opt_list in itertools.product(*java_opt_set_arrays):
            for opt_list in itertools.product(*opt_set_arrays):
                ensure_spark_stopped_on_slaves(slaves_list)
                results_token = "results: "
                # TODO(andy): Add a timout on the subprocess.
                cmd = "%s %s -cp %s %s %s %s" % (config.SCALA_CMD, " ".join(java_opt_list),
                    scala_cmd_classpath, test_cmd, config.SPARK_CLUSTER_URL, " ".join(opt_list))
                print("\nrunning command: %s\n" % cmd)
                output = Popen(cmd, stdout=PIPE, shell=True, env=new_env).stdout.read()
                if results_token not in output:
                    print("Test did not produce expected results. Output was:")
                    print(output)
                    sys.exit(1)
                result_line = filter(lambda x: results_token in x, output.split("\n"))[0]

                result_list = result_line.replace(results_token, "").split(",")
                assert len(result_list) > config.IGNORED_TRIALS, ("Expecting at least %s results "
                    "but only found %s" % (config.IGNORED_TRIALS + 1, len(result_list)))
                result_list = result_list[config.IGNORED_TRIALS:]
                result_first = result_list[0]
                result_last = result_list[len(result_list) - 1]

                # TODO(andy): For even cardinality lists, return average of middle two elts.
                result_list = sorted([float(x) for x in result_list])
                result_med = result_list[len(result_list)/2]
                result_std = sqrt(variance(result_list))
                result_min = min(result_list)
                result_string = "%s, %s, %s, %.3f, %s, %s, %s" % (short_name, " ".join(opt_list), 
                    result_med, result_std, result_min, result_first, result_last)
                print(result_string)
                out_file.write(result_string + "\n")
                sys.stdout.flush()
                out_file.flush()

    print("\nFinished running %d tests in %s.\nSee CSV output in %s" %
        (num_tests_to_run, test_group_name, output_filename))
    print(output_divider_string)

# Run all Spark and/or Shark tests specified in the Config file.
scala_cmd_classpath = "%s/spark-tests/target/spark-perf-tests-assembly.jar" % proj_dir
if has_spark_tests:
    run_tests(scala_cmd_classpath, config.SPARK_TESTS, "Spark-Tests", config.SPARK_OUTPUT_FILENAME)
if has_shark_tests:
    scala_cmd_classpath = (scala_cmd_classpath +
        ":%s/shark-tests/target/shark-perf-tests-assembly.jar" % proj_dir)
    # Set up the 'perf_test_warehouses'.
    if os.path.isdir(perf_test_warehouses_dir):
        clear_dir(perf_test_warehouses_dir, ["localhost"])
    else:
        os.mkdir(perf_test_warehouses_dir)

    # Prepend the path to the jar for Hive builtin functions to the classpath that will be passed to
    # config.SCALA_CMD in run_tests(). This is necessary because Hive's SessionState uses metadata
    # from that jar's META-INF/class-info.xml to register the builtin functions.
    # For more context, see org.apache.hadoop.hive.ql.session.SessionState's constructor and
    # org.apache.hadoop.hive.ql.exec.FunctionRegistry#registerFunctionsFromPluginJar().
    run_tests(scala_cmd_classpath, config.SHARK_TESTS, "Shark-Tests", config.SHARK_OUTPUT_FILENAME)

print("All tests have finished running. Stopping Spark standalone cluster ...")
run_cmd("%s/spark/bin/stop-all.sh" % proj_dir)

print("Finished running all tests.")
